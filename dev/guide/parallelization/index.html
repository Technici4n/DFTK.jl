<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Timings and parallelization · DFTK.jl</title><link rel="canonical" href="https://docs.dftk.org/stable/guide/parallelization/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="DFTK.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">DFTK.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../school2022/">DFTK School 2022</a></li><li><span class="tocitem">Getting started</span><ul><li><a class="tocitem" href="../installation/">Installation</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li><a class="tocitem" href="../input_output/">Input and output formats</a></li><li class="is-active"><a class="tocitem" href>Timings and parallelization</a><ul class="internal"><li><a class="tocitem" href="#Timing-measurements"><span>Timing measurements</span></a></li><li><a class="tocitem" href="#Rough-timing-estimates"><span>Rough timing estimates</span></a></li><li><a class="tocitem" href="#Options-for-parallelization"><span>Options for parallelization</span></a></li><li><a class="tocitem" href="#MPI-based-parallelism"><span>MPI-based parallelism</span></a></li><li><a class="tocitem" href="#Thread-based-parallelism"><span>Thread-based parallelism</span></a></li><li><a class="tocitem" href="#Advanced-threading-tweaks"><span>Advanced threading tweaks</span></a></li></ul></li><li><a class="tocitem" href="../periodic_problems/">Introduction to periodic problems</a></li><li><a class="tocitem" href="../density_functional_theory/">Density-functional theory</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../../examples/metallic_systems/">Temperature and metallic systems</a></li><li><a class="tocitem" href="../../examples/pymatgen/">Creating supercells with pymatgen</a></li><li><a class="tocitem" href="../../examples/ase/">Creating slabs with ASE</a></li><li><a class="tocitem" href="../../examples/collinear_magnetism/">Collinear spin and magnetic systems</a></li><li><a class="tocitem" href="../../examples/geometry_optimization/">Geometry optimization</a></li><li><a class="tocitem" href="../../examples/scf_callbacks/">Monitoring self-consistent field calculations</a></li><li><a class="tocitem" href="../../examples/scf_checkpoints/">Saving SCF results on disk and SCF checkpoints</a></li><li><a class="tocitem" href="../../examples/polarizability/">Polarizability by linear response</a></li><li><a class="tocitem" href="../../examples/gross_pitaevskii/">Gross-Pitaevskii equation in one dimension</a></li><li><a class="tocitem" href="../../examples/gross_pitaevskii_2D/">Gross-Pitaevskii equation with external magnetic field</a></li><li><a class="tocitem" href="../../examples/cohen_bergstresser/">Cohen-Bergstresser model</a></li><li><a class="tocitem" href="../../examples/arbitrary_floattype/">Arbitrary floating-point types</a></li><li><a class="tocitem" href="../../examples/forwarddiff/">Polarizability using automatic differentiation</a></li><li><a class="tocitem" href="../../examples/custom_solvers/">Custom solvers</a></li><li><a class="tocitem" href="../../examples/custom_potential/">Custom potential</a></li><li><a class="tocitem" href="../../examples/wannier90/">Wannierization using Wannier90</a></li><li><a class="tocitem" href="../../examples/error_estimates_forces/">Practical error bounds for the forces</a></li></ul></li><li><span class="tocitem">Advanced topics</span><ul><li><a class="tocitem" href="../../advanced/conventions/">Notation and conventions</a></li><li><a class="tocitem" href="../../advanced/data_structures/">Data structures</a></li><li><a class="tocitem" href="../../advanced/useful_formulas/">Useful formulas</a></li><li><a class="tocitem" href="../../advanced/symmetries/">Crystal symmetries</a></li></ul></li><li><a class="tocitem" href="../../api/">API reference</a></li><li><a class="tocitem" href="../../publications/">Publications</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Getting started</a></li><li class="is-active"><a href>Timings and parallelization</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Timings and parallelization</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaMolSim/DFTK.jl/blob/master/docs/src/guide/parallelization.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Timings-and-parallelization"><a class="docs-heading-anchor" href="#Timings-and-parallelization">Timings and parallelization</a><a id="Timings-and-parallelization-1"></a><a class="docs-heading-anchor-permalink" href="#Timings-and-parallelization" title="Permalink"></a></h1><p>This section summarizes the options DFTK offers to monitor and influence performance of the code.</p><h2 id="Timing-measurements"><a class="docs-heading-anchor" href="#Timing-measurements">Timing measurements</a><a id="Timing-measurements-1"></a><a class="docs-heading-anchor-permalink" href="#Timing-measurements" title="Permalink"></a></h2><p>By default DFTK uses <a href="https://github.com/KristofferC/TimerOutputs.jl">TimerOutputs.jl</a> to record timings, memory allocations and the number of calls for selected routines inside the code. These numbers are accessible in the object <code>DFTK.timer</code>. Since the timings are automatically accumulated inside this datastructure, any timing measurement should first reset this timer before running the calculation of interest.</p><p>For example to measure the timing of an SCF:</p><pre><code class="language-julia">DFTK.reset_timer!(DFTK.timer)
scfres = self_consistent_field(basis, tol=1e-8)

DFTK.timer</code></pre><pre class="documenter-example-output"> ────────────────────────────────────────────────────────────────────────────────
                                        Time                    Allocations      
                               ───────────────────────   ────────────────────────
       Tot / % measured:            1.19s /  20.2%           93.4MiB /  39.2%    

 Section               ncalls     time    %tot     avg     alloc    %tot      avg
 ────────────────────────────────────────────────────────────────────────────────
 self_consistent_field      1    239ms   99.8%   239ms   36.3MiB   99.0%  36.3MiB
   LOBPCG                  15    150ms   62.6%  10.0ms   12.7MiB   34.6%   865KiB
     DftHamiltonian...     55   79.9ms   33.3%  1.45ms   4.37MiB   11.9%  81.4KiB
       local+kinetic      351   74.0ms   30.8%   211μs    274KiB    0.7%     800B
       nonlocal            55   1.79ms    0.7%  32.5μs   0.99MiB    2.7%  18.3KiB
     ortho! X vs Y         65   36.9ms   15.4%   568μs   1.62MiB    4.4%  25.4KiB
       ortho!             135   31.3ms   13.1%   232μs   1.03MiB    2.8%  7.83KiB
     ortho!                15   14.8ms    6.2%   988μs   93.3KiB    0.2%  6.22KiB
     rayleigh_ritz         40   6.45ms    2.7%   161μs   1.26MiB    3.4%  32.1KiB
     preconditioning       55   1.37ms    0.6%  24.9μs   35.9KiB    0.1%     669B
   compute_density          5   42.4ms   17.7%  8.48ms   3.47MiB    9.5%   710KiB
     symmetrize_ρ           5   29.5ms   12.3%  5.89ms   2.78MiB    7.6%   570KiB
       accumulate_o...      5   27.8ms   11.6%  5.56ms    338KiB    0.9%  67.6KiB
   energy_hamiltonian      11   41.3ms   17.2%  3.76ms   16.3MiB   44.4%  1.48MiB
     ene_ops               11   37.4ms   15.6%  3.40ms   10.0MiB   27.3%   933KiB
       ene_ops: xc         11   30.9ms   12.9%  2.81ms   3.66MiB   10.0%   341KiB
       ene_ops: har...     11   4.01ms    1.7%   365μs   4.96MiB   13.5%   462KiB
       ene_ops: non...     11    979μs    0.4%  89.0μs    163KiB    0.4%  14.8KiB
       ene_ops: kin...     11    521μs    0.2%  47.3μs    103KiB    0.3%  9.35KiB
       ene_ops: local      11    507μs    0.2%  46.1μs   1.01MiB    2.7%  93.8KiB
   QR orthonormaliz...     15    383μs    0.2%  25.5μs    238KiB    0.6%  15.8KiB
   χ0Mixing                 5   79.9μs    0.0%  16.0μs   20.5KiB    0.1%  4.11KiB
 guess_density              1    588μs    0.2%   588μs    366KiB    1.0%   366KiB
 ────────────────────────────────────────────────────────────────────────────────</pre><p>The output produced when printing or displaying the <code>DFTK.timer</code> now shows a nice table summarising total time and allocations as well as a breakdown over individual routines.</p><div class="admonition is-info"><header class="admonition-header">Timing measurements and stack traces</header><div class="admonition-body"><p>Timing measurements have the unfortunate disadvantage that they alter the way stack traces look making it sometimes harder to find errors when debugging. For this reason timing measurements can be disabled completely (i.e. not even compiled into the code) by setting the environment variable <code>DFTK_TIMING</code> to <code>&quot;0&quot;</code> or <code>&quot;false&quot;</code>. For this to take effect recompiling all DFTK (including the precompile cache) is needed.</p></div></div><h2 id="Rough-timing-estimates"><a class="docs-heading-anchor" href="#Rough-timing-estimates">Rough timing estimates</a><a id="Rough-timing-estimates-1"></a><a class="docs-heading-anchor-permalink" href="#Rough-timing-estimates" title="Permalink"></a></h2><p>A very (very) rough estimate of the time per SCF step (in seconds) can be obtained with the following function. The function assumes that FFTs are the limiting operation and that no parallelisation is employed.</p><pre><code class="language-julia">function estimate_time_per_scf_step(basis::PlaneWaveBasis)
    # Super rough figure from various tests on cluster, laptops, ... on a 128^3 FFT grid.
    time_per_FFT_per_grid_point = 30 #= ms =# / 1000 / 128^3

    (time_per_FFT_per_grid_point
     * prod(basis.fft_size)
     * length(basis.kpoints)
     * div(basis.model.n_electrons, DFTK.filled_occupation(basis.model), RoundUp)
     * 8  # mean number of FFT steps per state per k-point per iteration
     )
end

&quot;Time per SCF (s):  $(estimate_time_per_scf_step(basis))&quot;</code></pre><pre class="documenter-example-output">&quot;Time per SCF (s):  0.008009033203124998&quot;</pre><h2 id="Options-for-parallelization"><a class="docs-heading-anchor" href="#Options-for-parallelization">Options for parallelization</a><a id="Options-for-parallelization-1"></a><a class="docs-heading-anchor-permalink" href="#Options-for-parallelization" title="Permalink"></a></h2><p>At the moment DFTK offers two ways to parallelize a calculation, firstly shared-memory parallelism using threading and secondly multiprocessing using MPI (via the <a href="https://github.com/JuliaParallel/MPI.jl">MPI.jl</a> Julia interface). MPI-based parallelism is currently only over <span>$k$</span>-points, such that it cannot be used for calculations with only a single <span>$k$</span>-point. Otherwise combining both forms of parallelism is possible as well.</p><p>The scaling of both forms of parallelism for a number of test cases is demonstrated in the following figure. These values were obtained using DFTK version 0.1.17 and Julia 1.6 and the precise scalings will likely be different depending on architecture, DFTK or Julia version. The rough trends should, however, be similar.</p><img src="../scaling.png" width=750 /><p>The MPI-based parallelization strategy clearly shows a superior scaling and should be preferred if available.</p><h2 id="MPI-based-parallelism"><a class="docs-heading-anchor" href="#MPI-based-parallelism">MPI-based parallelism</a><a id="MPI-based-parallelism-1"></a><a class="docs-heading-anchor-permalink" href="#MPI-based-parallelism" title="Permalink"></a></h2><p>Currently DFTK uses MPI to distribute on <span>$k$</span>-points only. This implies that calculations with only a single <span>$k$</span>-point cannot use make use of this. For details on setting up and configuring MPI with Julia see the <a href="https://juliaparallel.github.io/MPI.jl/stable/configuration">MPI.jl documentation</a>.</p><ol><li><p>First disable all threading inside DFTK, by adding the following to your script running the DFTK calculation:</p><pre><code class="language-julia">using DFTK
disable_threading()</code></pre></li><li><p>Run Julia in parallel using the <code>mpiexecjl</code> wrapper script from MPI.jl:</p><pre><code class="language-sh">mpiexecjl -np 16 julia myscript.jl</code></pre><p>In this <code>-np 16</code> tells MPI to use 16 processes and <code>-t 1</code> tells Julia to use one thread only.   Notice that we use <code>mpiexecjl</code> to automatically select the <code>mpiexec</code> compatible with the MPI version used by MPI.jl.</p></li></ol><p>As usual with MPI printing will be garbled. You can use</p><pre><code class="language-julia">DFTK.mpi_master() || (redirect_stdout(); redirect_stderr())</code></pre><p>at the top of your script to disable printing on all processes but one.</p><div class="admonition is-info"><header class="admonition-header">MPI-based parallelism not fully supported</header><div class="admonition-body"><p>While standard procedures (such as the SCF or band structure calculations) fully support MPI, not all routines of DFTK are compatible with MPI yet and will throw an error when being called in an MPI-parallel run. In most cases there is no intrinsic limitation it just has not yet been implemented. If you require MPI in one of our routines, where this is not yet supported, feel free to open an issue on github or otherwise get in touch.</p></div></div><h2 id="Thread-based-parallelism"><a class="docs-heading-anchor" href="#Thread-based-parallelism">Thread-based parallelism</a><a id="Thread-based-parallelism-1"></a><a class="docs-heading-anchor-permalink" href="#Thread-based-parallelism" title="Permalink"></a></h2><p>Threading in DFTK currently happens on multiple layers distributing the workload over different <span>$k$</span>-points, bands or within an FFT or BLAS call between threads. At its current stage our scaling for thread-based parallelism is worse compared MPI-based and therefore the parallelism described here should only be used if no other option exists. To use thread-based parallelism proceed as follows:</p><ol><li><p>Ensure that threading is properly setup inside DFTK by adding to the script running the DFTK calculation:</p><pre><code class="language-julia">using DFTK
setup_threading()</code></pre><p>This disables FFT threading and sets the number of BLAS threads to the number of Julia threads.</p></li><li><p>Run Julia passing the desired number of threads using the flag <code>-t</code>:</p><pre><code class="language-sh">julia -t 8 myscript.jl</code></pre></li></ol><p>For some cases (e.g. a single <span>$k$</span>-point, fewish bands and a large FFT grid) it can be advantageous to add threading inside the FFTs as well. One example is the Caffeine calculation in the above scaling plot. In order to do so just call <code>setup_threading(n_fft=2)</code>, which will select two FFT threads. More than two FFT threads is rarely useful.</p><h2 id="Advanced-threading-tweaks"><a class="docs-heading-anchor" href="#Advanced-threading-tweaks">Advanced threading tweaks</a><a id="Advanced-threading-tweaks-1"></a><a class="docs-heading-anchor-permalink" href="#Advanced-threading-tweaks" title="Permalink"></a></h2><p>The default threading setup done by <code>setup_threading</code> is to select one FFT thread and the same number of BLAS and Julia threads. This section provides some info in case you want to change these defaults.</p><h3 id="BLAS-threads"><a class="docs-heading-anchor" href="#BLAS-threads">BLAS threads</a><a id="BLAS-threads-1"></a><a class="docs-heading-anchor-permalink" href="#BLAS-threads" title="Permalink"></a></h3><p>All BLAS calls in Julia go through a parallelized OpenBlas or MKL (with <a href="https://github.com/JuliaComputing/MKL.jl">MKL.jl</a>. Generally threading in BLAS calls is far from optimal and the default settings can be pretty bad. For example for CPUs with hyper threading enabled, the default number of threads seems to equal the number of <em>virtual</em> cores. Still, BLAS calls typically take second place in terms of the share of runtime they make up (between 10% and 20%). Of note many of these do not take place on matrices of the size of the full FFT grid, but rather only in a subspace (e.g. orthogonalization, Rayleigh-Ritz, ...) such that parallelization is either anyway disabled by the BLAS library or not very effective. To <strong>set the number of BLAS threads</strong> use</p><pre><code class="language-none">using LinearAlgebra
BLAS.set_num_threads(N)</code></pre><p>where <code>N</code> is the number of threads you desire. To <strong>check the number of BLAS threads</strong> currently used, you can use</p><pre><code class="language-none">Int(ccall((BLAS.@blasfunc(openblas_get_num_threads), BLAS.libblas), Cint, ()))</code></pre><p>or (from Julia 1.6) simply <code>BLAS.get_num_threads()</code>.</p><h3 id="Julia-threads"><a class="docs-heading-anchor" href="#Julia-threads">Julia threads</a><a id="Julia-threads-1"></a><a class="docs-heading-anchor-permalink" href="#Julia-threads" title="Permalink"></a></h3><p>On top of BLAS threading DFTK uses Julia threads (<code>Thread.@threads</code>) in a couple of places to parallelize over <span>$k$</span>-points (density computation) or bands (Hamiltonian application). The number of threads used for these aspects is controlled by the flag <code>-t</code> passed to Julia or the <em>environment variable</em> <code>JULIA_NUM_THREADS</code>. To <strong>check the number of Julia threads</strong> use <code>Threads.nthreads()</code>.</p><h3 id="FFT-threads"><a class="docs-heading-anchor" href="#FFT-threads">FFT threads</a><a id="FFT-threads-1"></a><a class="docs-heading-anchor-permalink" href="#FFT-threads" title="Permalink"></a></h3><p>Since FFT threading is only used in DFTK inside the regions already parallelized by Julia threads, setting FFT threads to something larger than <code>1</code> is rarely useful if a sensible number of Julia threads has been chosen. Still, to explicitly <strong>set the FFT threads</strong> use</p><pre><code class="language-none">using FFTW
FFTW.set_num_threads(N)</code></pre><p>where <code>N</code> is the number of threads you desire. By default no FFT threads are used, which is almost always the best choice.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../input_output/">« Input and output formats</a><a class="docs-footer-nextpage" href="../periodic_problems/">Introduction to periodic problems »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 7 July 2022 14:38">Thursday 7 July 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
